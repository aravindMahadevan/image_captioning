{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random as random\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "karpathy_json_path = 'data/dataset_flickr8k.json'\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training splits across all data set \n",
    "with open(karpathy_json_path, 'r') as j:\n",
    "        flickr8k_datasplit = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "dataset\n"
     ]
    }
   ],
   "source": [
    "for key in flickr8k_datasplit:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = 'data/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_splits(image_folder, image_splits, max_caption_len, min_word_freq):\n",
    "    '''\n",
    "        method will return three lists containing the training, validation, and testing \n",
    "        file paths and captions. Method will also return the word map that will be used for captioning\n",
    "        images. \n",
    "        Parameters: \n",
    "            image_folder: location of the images in the data set, either MSCOCO, Flickr8K, Flickr30k \n",
    "            \n",
    "            image_splits: dictionary that contains the captions for each image and indicates whether \n",
    "            image part of training, validation, or testing set\n",
    "            \n",
    "            max_caption_len: threshold for maximum caption length \n",
    "            \n",
    "            min_word_freq: threshold that determines whether a word will be in word map or not. \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            train_img_caps: list of tuples containing the training image file path and caption\n",
    "            val_img_caps: list of tuples containing the validation image file path and caption\n",
    "            test_img_caps: list of tuples containing the testing image file path and caption\n",
    "        '''\n",
    "    #storing tuple of path to img and the caption\n",
    "    word_freq = Counter()\n",
    "    train_img_caps = []\n",
    "    val_img_caps = []\n",
    "    test_img_caps = [] \n",
    "    num_train_img, num_val_img, num_test_img = 0, 0, 0 \n",
    "    for img in image_splits['images']:\n",
    "        img_captions = []\n",
    "        for word in img['sentences']:\n",
    "            #check if the caption length is not to long\n",
    "            if len(word['tokens']) <= max_caption_len:\n",
    "                img_captions.append(word['tokens'])\n",
    "            # Update word frequency\n",
    "            word_freq.update(word['tokens'])\n",
    "\n",
    "        #if caption is of length zero move to next image \n",
    "        if not len(img_captions): \n",
    "            continue \n",
    "\n",
    "        img_file_path = os.path.join(image_folder, img['filename'])\n",
    "        #save corresponding files and captions \n",
    "        if img['split'] == 'train':\n",
    "            train_img_caps.append((img_file_path, img_captions))\n",
    "            num_train_img+=1\n",
    "        elif img['split'] == 'val':\n",
    "            val_img_caps.append((img_file_path, img_captions))\n",
    "            num_val_img+=1 \n",
    "        elif img['split'] == 'test':\n",
    "            test_img_caps.append((img_file_path, img_captions))\n",
    "            num_test_img+=1\n",
    "    \n",
    "    #create a limited vocabulary and don't include any word that hasn't appeared \n",
    "    #min_word_freq times\n",
    "    words = [w for w in word_freq if word_freq[w] >= min_word_freq]\n",
    "    min_words = [w for w in word_freq if word_freq[w] < min_word_freq]\n",
    "    word_map = {word: i+1 for i, word in enumerate(words)}\n",
    "    #specify start token, end token, unknown token, and padding token \n",
    "    word_map['<START>'] = len(word_map) + 1 \n",
    "    word_map['<END>'] = len(word_map) + 1\n",
    "    word_map['<UNK>'] = len(word_map) + 1\n",
    "    word_map['<PAD>'] = 0\n",
    "    \n",
    "    print(\"Number of training images: {0}\".format(num_train_img))\n",
    "    print(\"Number of validation images: {0}\".format(num_val_img))\n",
    "    print(\"Number of testing images: {0}\".format(num_test_img))\n",
    "    return train_img_caps, val_img_caps, test_img_caps, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 6000\n",
      "Number of validation images: 1000\n",
      "Number of testing images: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, word_map = get_train_val_test_splits(image_folder, flickr8k_datasplit, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, split, word_map, base_file_name, captions_per_image):\n",
    "    output_folder = 'data/'\n",
    "    encoded_captions = []\n",
    "    encoded_captions_length = []\n",
    "    start_token = word_map['<START>']\n",
    "    end_token = word_map['<END>']\n",
    "    unknown_token = word_map['<UNK>']\n",
    "    padding_token = word_map['<PAD>']\n",
    "    training_data_file = os.path.join(output_folder, base_file_name + '_' + split + '_images.hdf5')\n",
    "    encoded_captions_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_captions.json')\n",
    "    encoded_captions_length_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_caption_lengths.json')\n",
    "    \n",
    "    print(\"Creating %s data set\" % split)\n",
    "    with h5py.File(os.path.join(output_folder, base_file_name + '_' + split + '_images' + '.hdf5'), 'a') as h:\n",
    "        images = h.create_dataset('images', (len(data), 3, 256, 256), dtype='uint8')\n",
    "        for image_idx ,(image_path, image_captions) in enumerate(data):\n",
    "            \n",
    "            #want to ensure that there are at least certain number of captions per image \n",
    "            #if current image has less than that threshold, then augement the captions\n",
    "            num_captions = len(image_captions)\n",
    "            if num_captions < captions_per_image: \n",
    "                chosen_captions = [random.choice(image_captions) for _ in range(captions_pe_image - num_captions)]\n",
    "                chosen_captions += image_captions\n",
    "            else:\n",
    "                chosen_captions = random.sample(image_captions, k = captions_per_image)\n",
    "            \n",
    "            #for the chosen captions, encode them\n",
    "            \n",
    "            for i, caption in enumerate(chosen_captions):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                encoded_caption = [word_map.get(w,unknown_token) for w in caption]\n",
    "                assert len(caption) == len(encoded_caption)\n",
    "                padding_for_caption = [padding_token for _ in range(MAX_LENGTH- len(caption))]\n",
    "                encoded_caption = [start_token] + encoded_caption + [end_token] + padding_for_caption\n",
    "                \n",
    "                encoded_captions.append(encoded_caption)\n",
    "                \n",
    "\n",
    "                assert len(encoded_caption) == MAX_LENGTH + 2 \n",
    "                encoded_captions_length.append(len(caption) + 2)\n",
    "            \n",
    "            #resize all images to be 256 x 256 \n",
    "            image = Image.open(image_path)\n",
    "            image_resize = image.resize((256, 256))\n",
    "            image_array = np.asarray(image_resize).transpose(2,0,1) #ensures that 3x256x256\n",
    "            images[image_idx] = image_array\n",
    "            \n",
    "            \n",
    "            assert len(image_array.shape) == 3\n",
    "            \n",
    "        h.attrs['cpi'] = captions_per_image \n",
    "        \n",
    "        print(\"Saving the encoded captions\")\n",
    "        #save the encoded captions and the encoded caption lengths to a json file \n",
    "        with open(encoded_captions_file, 'w') as j:\n",
    "            json.dump(encoded_captions, j)\n",
    "\n",
    "        with open(encoded_captions_length_file, 'w') as j:\n",
    "            json.dump(encoded_captions_length, j)\n",
    "        \n",
    "        print(\"Done creating the dataset for split \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(test_data, 'test', word_map, 'flickr8k', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, folder, name, split, transform=None):\n",
    "        '''\n",
    "            Create a data set class that will be used when passing into the data loader. \n",
    "        '''\n",
    "        self.split = split\n",
    "        \n",
    "        self.file = h5py.File(os.path.join(folder, name + '_' + self.split + '_images.hdf5'))\n",
    "        self.images = self.file['images']\n",
    "        \n",
    "        self.cpi = self.file.attrs['cpi']\n",
    "        \n",
    "        # load captions\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_captions.json'), 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "            \n",
    "        # load captions' lenghts\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_caption_lengths.json'), 'r') as f:\n",
    "            self.lengths = json.load(f)\n",
    "        \n",
    "                        \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.FloatTensor(self.images[idx // self.cpi] / 255.0)\n",
    "        \n",
    "        #TODO: not using standard formulation of mean=0, std=1\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        image = normalize(image)\n",
    "        \n",
    "        caption, caption_length = torch.LongTensor(self.captions[idx]), torch.LongTensor([self.lengths[idx]])\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            return image, caption, caption_length\n",
    "        else:\n",
    "            start = self.cpi * (idx // self.cpi)\n",
    "            end = start + self.cpi\n",
    "            \n",
    "            captions = torch.LongTensor(self.captions[start:end])\n",
    "            \n",
    "            \n",
    "            return image, caption, caption_length, captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset('data', 'flickr8k', 'train')\n",
    "val_set = MyDataset('data', 'flickr8k', 'val')\n",
    "test_set = MyDataset('data', 'flickr8k', 'test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16,shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = tv.models.resnet101(pretrained=True)\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((dim_size, dim_size))\n",
    "        \n",
    "        #TODO: decided not to fine tune blocks 2-4\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # [batch_size, encoded_dim_size, encoded_dim_size, 2048]\n",
    "        return self.pool(self.resnet(images)).permute(0, 2, 3, 1) \n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_encoder, dim_decoder, dim_attention):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_encoder = nn.Linear(dim_encoder, dim_attention)\n",
    "        self.attention_decoder = nn.Linear(dim_decoder, dim_attention)\n",
    "        self.both = nn.Linear(dim_attention, 1)\n",
    "    \n",
    "    def forward(self, out_encoder, hidden_decoder):\n",
    "        attention_encoder = self.attention_encoder(out_encoder)\n",
    "        attention_decoder = self.attention_decoder(hidden_decoder)\n",
    "        \n",
    "        weights = self.both(torch.relu(attention_encoder + attention_decoder.unsqueeze(1))).squeeze(2)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        \n",
    "        out = torch.sum((out_encoder * weights.unsqueeze(2)), dim=1)\n",
    "        \n",
    "        return out, weights \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_attention, dim_embed, dim_decoder, vocab_size, dim_encoder=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.dim_attention = dim_attention\n",
    "        self.dim_embed = dim_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.attention = Attention(dim_encoder, dim_decoder, dim_attention)\n",
    "        self.embed = nn.Embedding(vocab_size, dim_embed)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.decode_lstm = nn.LSTMCell(dim_embed + dim_encoder, dim_decoder, bias=True)\n",
    "        self.h_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.c_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.f = nn.Linear(dim_decoder, dim_encoder)\n",
    "        \n",
    "        self.fc1 = nn.Linear(dim_decoder, vocab_size)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_hidden(self, out_encoder):\n",
    "        out = out_encoder.mean(dim=1)\n",
    "        \n",
    "        return self.h_init(out), self.c_init(out)\n",
    "    \n",
    "    def forward(self, out_encoder, captions, lengths):\n",
    "        batch_size = out_encoder.size(0)\n",
    "        dim_encoder = out_encoder.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        out_encoder = out_encoder.view(batch_size, -1, dim_encoder)\n",
    "        pixels = out_encoder.size(1)\n",
    "        \n",
    "        lengths, ind = lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        out_encoder = out_encoder[ind]\n",
    "        captions = captions[ind]\n",
    "        \n",
    "        embed = self.embed(captions)\n",
    "        \n",
    "        # init hidden state\n",
    "        h, c = self.init_hidden(out_encoder)\n",
    "        \n",
    "        lengths = (lengths-1).tolist()\n",
    "        \n",
    "        predict = torch.zeros(batch_size, max(lengths), vocab_size).to(device)\n",
    "        weights = torch.zeros(batch_size, max(lengths), pixels).to(device)\n",
    "        \n",
    "        for time_step in range(max(lengths)):\n",
    "            batch_t = sum([i > time_step for i in lengths])\n",
    "            \n",
    "            weighted_encoder, alpha = self.attention(out_encoder[:batch_t], h[:batch_t])\n",
    "            \n",
    "            sig = torch.sigmoid(self.f(h[:batch_t]))\n",
    "            weighted_encoder = sig * weighted_encoder\n",
    "            \n",
    "            h, c = self.decode_lstm(torch.cat([embed[:batch_t, time_step, :], weighted_encoder], dim=1),\n",
    "                                    (h[:batch_t], c[:batch_t]))\n",
    "            \n",
    "            output = self.fc1(self.drop(h))\n",
    "            predict[:batch_t, time_step, :] = output\n",
    "            weights[:batch_t, time_step, :] = alpha\n",
    "            \n",
    "        return predict, captions, lengths, weights, ind\n",
    "    \n",
    "    def caption(self, encoded_img_features, word_map, beam_size=5):\n",
    "        '''\n",
    "            In this method, given the encoded image features, we will do beam \n",
    "            search to obtain the final captions for the image. \n",
    "        '''\n",
    "        print(encoded_img_features.shape)\n",
    "        \n",
    "        enc_img_size, encoder_dimension = encoded_img_features.size(1), encoded_img_features.size(3)\n",
    "        encoded_img_features = encoded_img_features.view(1, -1, encoder_dimension)\n",
    "        total_pixels = encoded_img_features.size(1)\n",
    "        \n",
    "        encoded_img_features = encoded_img_features.expand(beam_size, total_pixels, encoder_dimension)\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        #initialize all sequences to only contain the start token at very beginning \n",
    "        captions =  torch.LongTensor([[word_map['<START>']]] * beam_size).to(device)\n",
    "\n",
    "        prev_words = captions\n",
    "        top_scores = torch.zeros(beam_size, 1).to(device)\n",
    "        \n",
    "        \n",
    "        # alpha weights \n",
    "        soft_weights = torch.ones(beam_size, 1, enc_img_size, enc_img_size).to(device)\n",
    "        \n",
    "        completed_captions = []\n",
    "        completed_captions_scores = []\n",
    "        completed_captions_weights = []\n",
    "        \n",
    "        current_iter = 1 \n",
    "        h,c = self.init_hidden(encoded_img_features)\n",
    "        \n",
    "        while True: \n",
    "#             import pdb; pdb.set_trace()\n",
    "            embedding = self.embed(prev_words).squeeze(1)\n",
    "            weighted_encoder, alpha = self.attention(encoded_img_features, h)\n",
    "            \n",
    "            sig = torch.sigmoid(self.f(h))\n",
    "            weighted_encoder = sig * weighted_encoder\n",
    "            \n",
    "            h,c = self.decode_lstm(torch.cat((embedding, weighted_encoder), dim=1), (h,c))\n",
    "            output = self.fc1(h)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "            \n",
    "            output = top_scores.expand_as(output) + output \n",
    "            assert output.size(1) == len(word_map)\n",
    "            if current_iter == 1: \n",
    "                top_scores, top_words = output[0].topk(beam_size, 0, True, True)\n",
    "            else:\n",
    "                top_scores, top_words = output.view(-1).topk(beam_size, 0, True, True)\n",
    "            prev_word_idxs, next_word_idxs = top_words / output.size(1), top_words % output.size(1)\n",
    "            \n",
    "            \n",
    "            captions = torch.cat((captions[prev_word_idxs], next_word_idxs.unsqueeze(1)), dim=1)\n",
    "            \n",
    "            \n",
    "            alpha = alpha.view(-1, enc_img_size, enc_img_size)\n",
    "            soft_weights = torch.cat((soft_weights[prev_word_idxs], alpha[prev_word_idxs].unsqueeze(1)), dim=1)\n",
    "\n",
    "            incomplete_idx = [idx for idx, next_word in enumerate(next_word_idxs) if next_word != word_map['<END>']]\n",
    "            complete_idx = list(set(range(len(next_word_idxs))) - set(incomplete_idx))\n",
    "            if len(complete_idx):\n",
    "                completed_captions.extend(captions[complete_idx].tolist())\n",
    "                completed_captions_scores.extend(top_scores[complete_idx])\n",
    "                completed_captions_weights.extend(soft_weights[complete_idx].tolist())\n",
    "            beam_size -= len(complete_idx)\n",
    "            \n",
    "            #done with beam search and can break\n",
    "            if beam_size == 0: \n",
    "                print(\"finished beam search\")\n",
    "                break \n",
    "            \n",
    "            captions = captions[incomplete_idx]\n",
    "            soft_weights = soft_weights[incomplete_idx]\n",
    "            h, c = h[prev_word_idxs[incomplete_idx]], c[prev_word_idxs[incomplete_idx]]\n",
    "            encoded_img_features = encoded_img_features[prev_word_idxs[incomplete_idx]]\n",
    "            top_scores = top_scores[incomplete_idx].unsqueeze(1)\n",
    "            prev_words = next_word_idxs[incomplete_idx].unsqueeze(1)\n",
    "\n",
    "            if current_iter > 50: \n",
    "                print(\"max iteration complete\")\n",
    "                break\n",
    "            \n",
    "            current_iter+=1 \n",
    "        \n",
    "        best_seq_idx = completed_captions_scores.index(max(completed_captions_scores))\n",
    "        chosen_caption, caption_weight = completed_captions[best_seq_idx], completed_captions_weights[best_seq_idx]\n",
    "        \n",
    "        \n",
    "        return chosen_caption, caption_weight,   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(decoder, encoder_image_features, smooth_weights = True):\n",
    "    caption, weights = decoder.caption(encoder_image_features, word_map)\n",
    "    idx2word = {v: k for k, v in word_map.items()}  \n",
    "\n",
    "    words = [idx2word[i] for i in caption]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if (i>50):\n",
    "            break\n",
    "        \n",
    "        plt.subplot(np.ceil(len(words)/5.), 5, (i+1))\n",
    "        plt.text(0, 1, '%s' % (words[i]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        current_weight = weights[i, :]\n",
    "        \n",
    "        if smooth_weights:\n",
    "            weight = skimage.transform.pyramid_expand(current_weight.numpy(), upscale=24, sigma=8)\n",
    "        else:\n",
    "            weight = skimage.transform.resize(current_weight.numpy(), [14*24, 14*24])        \n",
    "        \n",
    "        if(i==0):\n",
    "            plt.imshow(weight, weight=0)\n",
    "        else:\n",
    "            plt.imshow(weight, weight=0.8)\n",
    "        \n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(dim_attention=512, dim_embed=512, dim_decoder=512, vocab_size=len(word_map)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(params=decoder.parameters(), lr=4e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "image = val_set[0][0].unsqueeze(0).to(device)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 14, 2048])\n",
      "finished beam search\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f363e18c6d84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-3d08f06653a7>\u001b[0m in \u001b[0;36mvisualize_attention\u001b[0;34m(decoder, encoder_image_features, smooth_weights)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackgroundcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcurrent_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2682\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2683\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2684\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2685\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5669\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5671\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5672\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         if (self._A.dtype != np.uint8 and\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36msafe_masked_invalid\u001b[0;34m(x, copy)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;31m# Note that the argument to `byteswap` is 'inplace',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGIAAABjCAYAAABg+dWrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAHwElEQVR4nO2dX0xTWR7Hvz8rAorgHwguVssoQlQEHAhB2Sy8YAayBkNWM0CyuGqIm8yD+jBZjVnfXHnQRMjGCSajLAmYjRJDZoeIITFqMqvoplbGRULWmDbq1Lilmwn+gfDdh5YupS1cltoey/kkN+ntOf3dc/u555Se/jhXSEITfRZFuwEaD4sjdJxXANIjdKxPhZ8ArJnckQgNTXr8C45MPtBDkyJoEYqgRSiCFqEIyou4e/cudu7ciZSUFKxatQqlpaXo7+/H6dOnkZSUhKSkJCQkJMBkMvn2t27d6ns9SWzYsAFbtmwJiF1eXo6EhAQkJSUhNTUVNTU1ePnyJQDg8OHDvnhLlixBXFycb7+ysjL8J0oyEtucefXqFd1uN1NSUtjR0cHx8XGOjo7yxo0bfPTokV/dS5cusbS0NGicW7ducdmyZYyPj+f9+/f9ysrKynjx4kWSpMvlYkVFBevq6gJinDp1ivX19UHbOE9875FSPWJkZAQXLlxAcXEx9u/fj6GhIQBAbW0tTCYTEhMTsWvXLuTl5RmO2dbWhurqalRVVaGtrS1kvRUrVmDPnj2wWq2GY2dlZaG6uhrXr1/H2NiY4dcFI+oiJiYmcPPmTdTV1cFisaC3txcnTpxAd3c3srOzYTKZ0NDQgJ6eHrhcrjnFHh0dxdWrV1FfX4/6+npcuXIFHz58CFr3zZs36OrqQlZWluH4drsdlZWVaGpqgtlsxrFjx/D48eM5tdEHozg0tbS0cN26ddy+fTvPnz/P169fB9R58uQJGxoauHbtWppMJu7evTtgSAg1NLW3tzM1NZVjY2N89+4dU1JS2NXV5SsvKytjYmIik5OTCYD5+fl8/vx5QJxQQ9NUBgcHefz4cZrNZhYWFrKvr2/G+l6MD00i8q2IOEVkIES5iEiziAyLiE1EPjd6ETx79gwulwsFBQXIy8vD6tWrA+ps3rwZly9fhsPhwMDAAF68eIEjR44Yit/W1oZ9+/Zh8eLFiI+PR01NTcDw1NzcDLfbDZvNBpfLBYfDYbT5flgsFuTn5yM3NxfDw8NwOp1zC8BZrmYAvwLwOYCBEOVVAHrg+bpeAuBekHohcTqdPHfuHLdt20aLxcKTJ09yaGgoZP2Wlhbm5ub6PResR9jtdi5atIjJyclMT09neno6ly9fzri4OF/Pm/phTZKtra0sKCjgxMSEX6xQPWJiYoK3b9/moUOHuHLlSlZUVLCjo4Nv376d6ZSnYrxHkLwN4N8zVKkG8Bdv4L8DWCEivzB6IaSlpeHo0aOw2Wy4du0aRkZGsGPHDhw4cACDg4M4e/as7yq12+3o7OxESUnJrHHb29uRnZ2Np0+fwmq1wmq1YmhoCGazGZ2dnUFf09DQAKfTie7ubkNt37hxIw4ePIjMzEzYbDb09vaitrYWCQkJRk//f9DAGA8gE6F7xHcAfjllvw9AEQ32iGC8f/+e9+7do8Ph4N69e5mRkcGlS5cyIyODjY2NdLvdfvWD9YicnBw2NzcHxG5qamJhYSHJwB5BkmfOnPGVTxKqR9y5c2eupzYd33tkaPZVRDIBfEcyN0jZ3wD8ieRd734fgK9JPpzqe+6XyIIgrLOvDgDrpuybAbwIQ9wFRThEdAP4rfevpxIAbpIvwxB3QTHrL3Qi0gmgHECqiDgAnAIQBwAkvwHwPTx/OQ0DGAXwu4/V2FhG/0IXXfQvdKqhRSiCFqEIkRLxU4SO8ynh955ESsQaeD6YfFtRUVGEDh1dROQhpp27d1sztZ4emhRBi1AELUIRtAhF0CIUQYtQBC1CEbQIRdAiFEGLUAQtQhG0CEXQIhRBi1AELUIRDIkQkS9E5Kk30fgPQcrLRcQtIlbv9sfwNzW2MZJOYwLwZwAV8CST9YtIN8kn06reIfnrj9DGBYGRHlEMYJjkv0h+AHAFnsRjTRgxImItAPuUfYf3uensEJFHItIjIluDlGtmwMhaHBLkuekJY/8AYCH5s4hUAbgOYFNAIJFGAI0AsH79+jk2NbYx0iNmTTIm+R+SP3sffw8gTkRSpwci2UqyiGRRWlraPJodexgR0Q9gk4h8JiJLAHwJT+KxDxFZIyLifVzsjfsm3I2NZWYdmkiOi8hXAG4AMAH4luSPInLYW/4NgN8A+L2IjAN4C+BLRiipNlaIVBJyAEVFRXzw4EFUjh1JROQhyVmTuPQ3a0XQIhRBi1AELUIRtAhF0CIUQYtQBC1CEbQIRdAiFEGLUAQtQhG0CEXQIhRBi1AELUIRtAhF0CIUQYtQBC1CEcKVhPx/r4as8WBkSerJJORKAFsA1IrI9JsxVMKT2bcJnky+C2FuZ8wTriTkea2GrAlfErLRRGVNCMKVhGykjl8SMoD3oVbgjzFyjFQyIsLISseGVkMm2QqgFQBE5IGRDLhPHRExlM4YliRk6NWQ5024kpD1asjzJGpJyCLS6B2qYhqj5xk1ERp/9BSHIkRFxGxTJrHAbDfJmk7ERRicMokFLgP4wmjlaPSIBfF/25z9Jll+REOEng4JQjREGJoOWWhEQ4S+OVQQoiHCyJTJgiPiIkiOA5icMvkngL+S/DHS7fjYeG+S9QOAHBFxiMjBGevrb9ZqoL9ZK4IWoQhahCJoEYqgRSiCFqEIWoQiaBGK8F8vO8YqsTamBgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_img = encoder(image)\n",
    "visualize_attention(decoder, encoded_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        img_features = encoder(images)\n",
    "        predict, captions, lengths, weights, ind = decoder(img_features, captions, lengths)\n",
    "        targets = captions[:,1:]\n",
    "        \n",
    "        predict = pack_padded_sequence(predict, lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "        \n",
    "        loss = criterion(predict, targets)\n",
    "        \n",
    "        loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "        \n",
    "        decoder_optimizer.zero_grad()\n",
    "#         encoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()*sum(lengths)\n",
    "        loss_num += sum(lengths)\n",
    "        if i % 100 == 0: \n",
    "            print(\"Epoch % d [%d/%d], Loss: %f\" % (epoch, i, len(train_loader), loss_sum/loss_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 [0/1875], Loss: 8.910374\n",
      "Epoch  0 [100/1875], Loss: 6.310832\n",
      "Epoch  0 [200/1875], Loss: 5.985870\n",
      "Epoch  0 [300/1875], Loss: 5.751834\n",
      "Epoch  0 [400/1875], Loss: 5.583942\n",
      "Epoch  0 [500/1875], Loss: 5.447486\n",
      "Epoch  0 [600/1875], Loss: 5.343106\n",
      "Epoch  0 [700/1875], Loss: 5.258333\n",
      "Epoch  0 [800/1875], Loss: 5.181723\n",
      "Epoch  0 [900/1875], Loss: 5.117714\n",
      "Epoch  0 [1000/1875], Loss: 5.058171\n",
      "Epoch  0 [1100/1875], Loss: 5.006650\n",
      "Epoch  0 [1200/1875], Loss: 4.961866\n",
      "Epoch  0 [1300/1875], Loss: 4.922001\n",
      "Epoch  0 [1400/1875], Loss: 4.884623\n",
      "Epoch  0 [1500/1875], Loss: 4.849919\n",
      "Epoch  0 [1600/1875], Loss: 4.818555\n",
      "Epoch  0 [1700/1875], Loss: 4.787220\n",
      "Epoch  0 [1800/1875], Loss: 4.759778\n",
      "Validate [0/313], Loss: 4.115417\n",
      "Validate [100/313], Loss: 4.133608\n",
      "Validate [200/313], Loss: 4.160437\n",
      "Validate [300/313], Loss: 4.169961\n",
      "Validate. BLEU-4: 0.122898, Loss: 4.169652\n",
      "Epoch  1 [0/1875], Loss: 4.115479\n",
      "Epoch  1 [100/1875], Loss: 4.176667\n",
      "Epoch  1 [200/1875], Loss: 4.154471\n",
      "Epoch  1 [300/1875], Loss: 4.149832\n",
      "Epoch  1 [400/1875], Loss: 4.145070\n",
      "Epoch  1 [500/1875], Loss: 4.139403\n",
      "Epoch  1 [600/1875], Loss: 4.126222\n",
      "Epoch  1 [700/1875], Loss: 4.120327\n",
      "Epoch  1 [800/1875], Loss: 4.110144\n",
      "Epoch  1 [900/1875], Loss: 4.107200\n",
      "Epoch  1 [1000/1875], Loss: 4.106719\n",
      "Epoch  1 [1100/1875], Loss: 4.103078\n",
      "Epoch  1 [1200/1875], Loss: 4.094412\n",
      "Epoch  1 [1300/1875], Loss: 4.086229\n",
      "Epoch  1 [1400/1875], Loss: 4.077856\n",
      "Epoch  1 [1500/1875], Loss: 4.069926\n",
      "Epoch  1 [1600/1875], Loss: 4.062366\n",
      "Epoch  1 [1700/1875], Loss: 4.056416\n",
      "Epoch  1 [1800/1875], Loss: 4.049456\n",
      "Validate [0/313], Loss: 4.218904\n",
      "Validate [100/313], Loss: 3.900290\n",
      "Validate [200/313], Loss: 3.907375\n",
      "Validate [300/313], Loss: 3.917597\n",
      "Validate. BLEU-4: 0.135379, Loss: 3.913263\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\n",
    "    bleu4 = validate(val_loader, encoder, decoder, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(decoder, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    \n",
    "    refs = []\n",
    "    hypos = []\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths, all_captions) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            images = encoder(images)\n",
    "            predict, captions, lengths, weights, ind = decoder(images, captions, lengths)\n",
    "            \n",
    "            targets = captions[:,1:]\n",
    "            \n",
    "            scores = predict.clone()\n",
    "            \n",
    "            predict = pack_padded_sequence(predict, lengths, batch_first=True).data\n",
    "            targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "            \n",
    "            loss = criterion(predict, targets)\n",
    "            \n",
    "            loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "                     \n",
    "            loss_sum += loss.item()*sum(lengths)\n",
    "            loss_num += sum(lengths)\n",
    "                     \n",
    "            if (i % 100) == 0:\n",
    "                print(\"Validate [%d/%d], Loss: %f\" % (i, len(val_loader), loss_sum/loss_num))\n",
    "            \n",
    "            all_captions = all_captions[ind]\n",
    "            for j in range(all_captions.shape[0]):\n",
    "                image_captions = all_captions[j].tolist()\n",
    "                image_captions = list(map(lambda c : [w for w in c if w not in {word_map['<START>'], \n",
    "                                                                                word_map['<PAD>']}],\n",
    "                                         image_captions))\n",
    "                refs.append(image_captions)\n",
    "            \n",
    "            pred = torch.max(scores, dim=2)[1].tolist()\n",
    "            temp = []\n",
    "            for j, p in enumerate(pred):\n",
    "                temp.append(pred[j][:lengths[j]])\n",
    "            pred = temp\n",
    "            hypos.extend(pred)\n",
    "            \n",
    "            assert len(refs) == len(hypos)\n",
    "            \n",
    "        bleu4 = corpus_bleu(refs, hypos)\n",
    "        \n",
    "        print(\"Validate. BLEU-4: %f, Loss: %f\" % (bleu4, loss_sum/loss_num))\n",
    "        \n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
