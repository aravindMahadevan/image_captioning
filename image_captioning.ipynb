{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random as random\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "karpathy_json_path = 'data/dataset_flickr8k.json'\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training splits across all data set \n",
    "with open(karpathy_json_path, 'r') as j:\n",
    "        flickr8k_datasplit = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "dataset\n"
     ]
    }
   ],
   "source": [
    "for key in flickr8k_datasplit:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_splits(image_folder, image_splits, max_caption_len, min_word_freq):\n",
    "    '''\n",
    "        method will return three lists containing the training, validation, and testing \n",
    "        file paths and captions. Method will also return the word map that will be used for captioning\n",
    "        images. \n",
    "        Parameters: \n",
    "            image_folder: location of the images in the data set, either MSCOCO, Flickr8K, Flickr30k \n",
    "            \n",
    "            image_splits: dictionary that contains the captions for each image and indicates whether \n",
    "            image part of training, validation, or testing set\n",
    "            \n",
    "            max_caption_len: threshold for maximum caption length \n",
    "            \n",
    "            min_word_freq: threshold that determines whether a word will be in word map or not. \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            train_img_caps: list of tuples containing the training image file path and caption\n",
    "            val_img_caps: list of tuples containing the validation image file path and caption\n",
    "            test_img_caps: list of tuples containing the testing image file path and caption\n",
    "        '''\n",
    "    #storing tuple of path to img and the caption\n",
    "    word_freq = Counter()\n",
    "    train_img_caps = []\n",
    "    val_img_caps = []\n",
    "    test_img_caps = [] \n",
    "    num_train_img, num_val_img, num_test_img = 0, 0, 0 \n",
    "    for img in image_splits['images']:\n",
    "        img_captions = []\n",
    "        for word in img['sentences']:\n",
    "            #check if the caption length is not to long\n",
    "            if len(word['tokens']) <= max_caption_len:\n",
    "                img_captions.append(word['tokens'])\n",
    "            # Update word frequency\n",
    "            word_freq.update(word['tokens'])\n",
    "\n",
    "        #if caption is of length zero move to next image \n",
    "        if not len(img_captions): \n",
    "            continue \n",
    "\n",
    "        img_file_path = os.path.join(image_folder, img['filename'])\n",
    "        #save corresponding files and captions \n",
    "        if img['split'] == 'train':\n",
    "            train_img_caps.append((img_file_path, img_captions))\n",
    "            num_train_img+=1\n",
    "        elif img['split'] == 'val':\n",
    "            val_img_caps.append((img_file_path, img_captions))\n",
    "            num_val_img+=1 \n",
    "        elif img['split'] == 'test':\n",
    "            test_img_caps.append((img_file_path, img_captions))\n",
    "            num_test_img+=1\n",
    "    \n",
    "    #create a limited vocabulary and don't include any word that hasn't appeared \n",
    "    #min_word_freq times\n",
    "    words = [w for w in word_freq if word_freq[w] >= min_word_freq]\n",
    "    min_words = [w for w in word_freq if word_freq[w] < min_word_freq]\n",
    "    word_map = {word: i+1 for i, word in enumerate(words)}\n",
    "    #specify start token, end token, unknown token, and padding token \n",
    "    word_map['<START>'] = len(word_map) + 1 \n",
    "    word_map['<END>'] = len(word_map) + 1\n",
    "    word_map['<UNK>'] = len(word_map) + 1\n",
    "    word_map['<PAD>'] = 0\n",
    "    \n",
    "    print(\"Number of training images: {0}\".format(num_train_img))\n",
    "    print(\"Number of validation images: {0}\".format(num_val_img))\n",
    "    print(\"Number of testing images: {0}\".format(num_test_img))\n",
    "    return train_img_caps, val_img_caps, test_img_caps, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 6000\n",
      "Number of validation images: 1000\n",
      "Number of testing images: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, word_map = get_train_val_test_splits(image_folder, flickr8k_datasplit, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, split, word_map, base_file_name, captions_per_image):\n",
    "    output_folder = 'data/'\n",
    "    encoded_captions = []\n",
    "    encoded_captions_length = []\n",
    "    start_token = word_map['<START>']\n",
    "    end_token = word_map['<END>']\n",
    "    unknown_token = word_map['<UNK>']\n",
    "    padding_token = word_map['<PAD>']\n",
    "    training_data_file = os.path.join(output_folder, base_file_name + '_' + split + '_images.hdf5')\n",
    "    encoded_captions_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_captions.json')\n",
    "    encoded_captions_length_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_caption_lengths.json')\n",
    "    \n",
    "    print(\"Creating %s data set\" % split)\n",
    "    with h5py.File(os.path.join(output_folder, base_file_name + '_' + split + '_images' + '.hdf5'), 'a') as h:\n",
    "        images = h.create_dataset('images', (len(data), 3, 256, 256), dtype='uint8')\n",
    "        for image_idx ,(image_path, image_captions) in enumerate(data):\n",
    "            \n",
    "            #want to ensure that there are at least certain number of captions per image \n",
    "            #if current image has less than that threshold, then augement the captions\n",
    "            num_captions = len(image_captions)\n",
    "            if num_captions < captions_per_image: \n",
    "                chosen_captions = [random.choice(image_captions) for _ in range(captions_pe_image - num_captions)]\n",
    "                chosen_captions += image_captions\n",
    "            else:\n",
    "                chosen_captions = random.sample(image_captions, k = captions_per_image)\n",
    "            \n",
    "            #for the chosen captions, encode them\n",
    "            \n",
    "            for i, caption in enumerate(chosen_captions):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                encoded_caption = [word_map.get(w,unknown_token) for w in caption]\n",
    "                assert len(caption) == len(encoded_caption)\n",
    "                padding_for_caption = [padding_token for _ in range(MAX_LENGTH- len(caption))]\n",
    "                encoded_caption = [start_token] + encoded_caption + [end_token] + padding_for_caption\n",
    "                \n",
    "                encoded_captions.append(encoded_caption)\n",
    "                \n",
    "\n",
    "                assert len(encoded_caption) == MAX_LENGTH + 2 \n",
    "                encoded_captions_length.append(len(caption) + 2)\n",
    "            \n",
    "            #resize all images to be 256 x 256 \n",
    "            image = Image.open(image_path)\n",
    "            image_resize = image.resize((256, 256))\n",
    "            image_array = np.asarray(image_resize).transpose(2,0,1) #ensures that 3x256x256\n",
    "            images[image_idx] = image_array\n",
    "            \n",
    "            \n",
    "            assert len(image_array.shape) == 3\n",
    "            \n",
    "        h.attrs['cpi'] = captions_per_image \n",
    "        \n",
    "        print(\"Saving the encoded captions\")\n",
    "        #save the encoded captions and the encoded caption lengths to a json file \n",
    "        with open(encoded_captions_file, 'w') as j:\n",
    "            json.dump(encoded_captions, j)\n",
    "\n",
    "        with open(encoded_captions_length_file, 'w') as j:\n",
    "            json.dump(encoded_captions_length, j)\n",
    "        \n",
    "        print(\"Done creating the dataset for split \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data set\n",
      "Saving the encoded captions\n",
      "Done creating the dataset for split \n"
     ]
    }
   ],
   "source": [
    "create_dataset(test_data, 'test', word_map, 'flickr8k', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, folder, name, split, transform=None):\n",
    "        '''\n",
    "            Create a data set class that will be used when passing into the data loader. \n",
    "        '''\n",
    "        self.split = split\n",
    "        \n",
    "        self.file = h5py.File(os.path.join(folder, name + '_' + self.split + '_images.hdf5'))\n",
    "        self.images = self.file['images']\n",
    "        \n",
    "        self.cpi = self.file.attrs['cpi']\n",
    "        \n",
    "        # load captions\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_captions.json'), 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "            \n",
    "        # load captions' lenghts\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_caption_lengths.json'), 'r') as f:\n",
    "            self.lengths = json.load(f)\n",
    "        \n",
    "                        \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.FloatTensor(self.images[idx // self.cpi] / 255.0)\n",
    "        \n",
    "        #TODO: not using standard formulation of mean=0, std=1\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        image = normalize(image)\n",
    "        \n",
    "        caption, caption_length = torch.LongTensor(self.captions[idx]), torch.LongTensor([self.lengths[idx]])\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            return image, caption, caption_length\n",
    "        else:\n",
    "            start = self.cpi * (idx // self.cpi)\n",
    "            end = start + self.cpi\n",
    "            \n",
    "            captions = torch.LongTensor(self.captions[start:end])\n",
    "            \n",
    "            \n",
    "            return image, caption, caption_length, captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = MyDataset('data', 'flickr8k', 'train')\n",
    "val_set = MyDataset('data', 'flickr8k', 'val')\n",
    "test_set = MyDataset('data', 'flickr8k', 'test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32,shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([32, 3, 256, 256]) torch.Size([32, 52]) torch.Size([32, 1]) torch.Size([32, 5, 52])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 52]) torch.Size([8, 1]) torch.Size([8, 5, 52])\n"
     ]
    }
   ],
   "source": [
    "for i, (images, captions, lengths, ac) in enumerate(val_loader):\n",
    "    print(images.shape, captions.shape, lengths.shape, ac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = tv.models.resnet101(pretrained=True)\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((dim_size, dim_size))\n",
    "        \n",
    "        #TODO: decided not to fine tune blocks 2-4\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.pool(self.resnet(images)).permute(0, 2, 3, 1)\n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_encoder, dim_decoder, dim_attention):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_encoder = nn.Linear(dim_encoder, dim_attention)\n",
    "        self.attention_decoder = nn.Linear(dim_decoder, dim_attention)\n",
    "        self.both = nn.Linear(dim_attention, 1)\n",
    "    \n",
    "    def forward(self, out_encoder, hidden_decoder):\n",
    "        attention_encoder = self.attention_encoder(out_encoder)\n",
    "        attention_decoder = self.attention_decoder(hidden_decoder)\n",
    "        \n",
    "        weights = self.both(torch.relu(attention_encoder + attention_decoder.unsqueeze(1))).squeeze(2)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        \n",
    "        out = torch.sum((out_encoder * weights.unsqueeze(2)), dim=1)\n",
    "        \n",
    "        return out, weights \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_attention, dim_embed, dim_decoder, vocab_size, dim_encoder=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.dim_attention = dim_attention\n",
    "        self.dim_embed = dim_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.attention = Attention(dim_encoder, dim_decoder, dim_attention)\n",
    "        self.embed = nn.Embedding(vocab_size, dim_embed)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.decode_lstm = nn.LSTMCell(dim_embed + dim_encoder, dim_decoder, bias=True)\n",
    "        self.h_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.c_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.f = nn.Linear(dim_decoder, dim_encoder)\n",
    "        \n",
    "        self.fc1 = nn.Linear(dim_decoder, vocab_size)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_hidden(self, out_encoder):\n",
    "        out = out_encoder.mean(dim=1)\n",
    "        \n",
    "        return self.h_init(out), self.c_init(out)\n",
    "    \n",
    "    def forward(self, out_encoder, captions, lengths):\n",
    "        batch_size = out_encoder.size(0)\n",
    "        dim_encoder = out_encoder.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        out_encoder = out_encoder.view(batch_size, -1, dim_encoder)\n",
    "        pixels = out_encoder.size(1)\n",
    "        \n",
    "        lengths, ind = lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        out_encoder = out_encoder[ind]\n",
    "        captions = captions[ind]\n",
    "        \n",
    "        embed = self.embed(captions)\n",
    "        \n",
    "        # init hidden state\n",
    "        h, c = self.init_hidden(out_encoder)\n",
    "        \n",
    "        lengths = (lengths-1).tolist()\n",
    "        \n",
    "        predict = torch.zeros(batch_size, max(lengths), vocab_size).to(device)\n",
    "        weights = torch.zeros(batch_size, max(lengths), pixels).to(device)\n",
    "        \n",
    "        for time_step in range(max(lengths)):\n",
    "            batch_t = sum([i > time_step for i in lengths])\n",
    "            \n",
    "            weighted_encoder, alpha = self.attention(out_encoder[:batch_t], h[:batch_t])\n",
    "            \n",
    "            sig = torch.sigmoid(self.f(h[:batch_t]))\n",
    "            weighted_encoder = sig * weighted_encoder\n",
    "            \n",
    "            h, c = self.decode_lstm(torch.cat([embed[:batch_t, time_step, :], weighted_encoder], dim=1),\n",
    "                                    (h[:batch_t], c[:batch_t]))\n",
    "            \n",
    "            output = self.fc1(self.drop(h))\n",
    "            predict[:batch_t, time_step, :] = output\n",
    "            weights[:batch_t, time_step, :] = alpha\n",
    "            \n",
    "        return predict, captions, lengths, weights, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /tmp/xdg-cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
      "100%|██████████| 170M/170M [00:02<00:00, 66.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(dim_attention=512, dim_embed=512, dim_decoder=512, vocab_size=len(word_map)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(params=decoder.parameters(), lr=4e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        \n",
    "        images = encoder(images)\n",
    "        predict, captions, lengths, weights, ind = decoder(images, captions, lengths)\n",
    "        \n",
    "        targets = captions[:,1:]\n",
    "        \n",
    "        predict,_ = pack_padded_sequence(predict, lengths, batch_first=True)\n",
    "        targets,_ = pack_padded_sequence(targets, lengths, batch_first=True)\n",
    "        \n",
    "        loss = criterion(predict, targets)\n",
    "        \n",
    "        loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "        \n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        loss_sum += loss.item()*sum(lengths)\n",
    "        loss_num += sum(lengths)\n",
    "        \n",
    "        if (i % 100) == 0:\n",
    "            print(\"Epoch % d [%d/%d], Loss: %f\" % (epoch, i, len(train_loader), loss_sum/loss_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    \n",
    "    refs = []\n",
    "    hypos = []\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths, all_captions) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            images = encoder(images)\n",
    "            predict, captions, lengths, weights, ind = decoder(images, captions, lengths)\n",
    "            \n",
    "            targets = captions[:,1:]\n",
    "            \n",
    "            scores = predict.clone()\n",
    "            \n",
    "            predict,_ = pack_padded_sequence(predict, lengths, batch_first=True)\n",
    "            targets,_ = pack_padded_sequence(targets, lengths, batch_first=True)\n",
    "            \n",
    "            loss = criterion(predict, targets)\n",
    "            \n",
    "            loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "                     \n",
    "            loss_sum += loss.item()*sum(lengths)\n",
    "            loss_num += sum(lengths)\n",
    "                     \n",
    "            if (i % 100) == 0:\n",
    "                print(\"Validate [%d/%d], Loss: %f\" % (i, len(val_laoder, loss_sum/loss_num)))\n",
    "            \n",
    "            all_captions = all_captions[ind]\n",
    "            for j in range(all_captions.shape[0]):\n",
    "                image_captions = all_captions[j].tolist()\n",
    "                image_captions = list(map(lambda c : [w for w in c if w not in {word_map['<START>'], \n",
    "                                                                                word_map['<PAD>']}],\n",
    "                                         image_captions))\n",
    "                refs.append(image_captions)\n",
    "            \n",
    "            _, pred = torch.max(scores, dim=2).tolist()\n",
    "            temp = []\n",
    "            for j, p in enumerate(pred):\n",
    "                temp.append(pred[j][:lengths[j]])\n",
    "            pred = temp\n",
    "            hypos.extend(pred)\n",
    "            \n",
    "            assert len(refs) == len(hypos)\n",
    "            \n",
    "        bleu4 = corpus_bleu(refs, hypos)\n",
    "        \n",
    "        print(\"Validate. BLEU-4: %f, Loss: %f\" % (bleu4, loss_sum/loss_num))\n",
    "        \n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 0. Got 14 and 19 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-f6967873fd90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbleu4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-fe80f0a5ed34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 14 and 19 in dimension 1 at ../aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "for epoch in range(120):\n",
    "    train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\n",
    "    bleu4 = validate(val_loader, encoder, decoder, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
