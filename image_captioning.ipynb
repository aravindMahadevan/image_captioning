{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random as random\n",
    "from PIL import Image \n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from torch.nn.utils.rnn import pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "karpathy_json_path = 'data/dataset_flickr8k.json'\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the training splits across all data set \n",
    "with open(karpathy_json_path, 'r') as j:\n",
    "        flickr8k_datasplit = json.load(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "dataset\n"
     ]
    }
   ],
   "source": [
    "for key in flickr8k_datasplit:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_folder = 'data/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_splits(image_folder, image_splits, max_caption_len, min_word_freq):\n",
    "    '''\n",
    "        method will return three lists containing the training, validation, and testing \n",
    "        file paths and captions. Method will also return the word map that will be used for captioning\n",
    "        images. \n",
    "        Parameters: \n",
    "            image_folder: location of the images in the data set, either MSCOCO, Flickr8K, Flickr30k \n",
    "            \n",
    "            image_splits: dictionary that contains the captions for each image and indicates whether \n",
    "            image part of training, validation, or testing set\n",
    "            \n",
    "            max_caption_len: threshold for maximum caption length \n",
    "            \n",
    "            min_word_freq: threshold that determines whether a word will be in word map or not. \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            train_img_caps: list of tuples containing the training image file path and caption\n",
    "            val_img_caps: list of tuples containing the validation image file path and caption\n",
    "            test_img_caps: list of tuples containing the testing image file path and caption\n",
    "        '''\n",
    "    #storing tuple of path to img and the caption\n",
    "    word_freq = Counter()\n",
    "    train_img_caps = []\n",
    "    val_img_caps = []\n",
    "    test_img_caps = [] \n",
    "    num_train_img, num_val_img, num_test_img = 0, 0, 0 \n",
    "    for img in image_splits['images']:\n",
    "        img_captions = []\n",
    "        for word in img['sentences']:\n",
    "            #check if the caption length is not to long\n",
    "            if len(word['tokens']) <= max_caption_len:\n",
    "                img_captions.append(word['tokens'])\n",
    "            # Update word frequency\n",
    "            word_freq.update(word['tokens'])\n",
    "\n",
    "        #if caption is of length zero move to next image \n",
    "        if not len(img_captions): \n",
    "            continue \n",
    "\n",
    "        img_file_path = os.path.join(image_folder, img['filename'])\n",
    "        #save corresponding files and captions \n",
    "        if img['split'] == 'train':\n",
    "            train_img_caps.append((img_file_path, img_captions))\n",
    "            num_train_img+=1\n",
    "        elif img['split'] == 'val':\n",
    "            val_img_caps.append((img_file_path, img_captions))\n",
    "            num_val_img+=1 \n",
    "        elif img['split'] == 'test':\n",
    "            test_img_caps.append((img_file_path, img_captions))\n",
    "            num_test_img+=1\n",
    "    \n",
    "    #create a limited vocabulary and don't include any word that hasn't appeared \n",
    "    #min_word_freq times\n",
    "    words = [w for w in word_freq if word_freq[w] >= min_word_freq]\n",
    "    min_words = [w for w in word_freq if word_freq[w] < min_word_freq]\n",
    "    word_map = {word: i+1 for i, word in enumerate(words)}\n",
    "    #specify start token, end token, unknown token, and padding token \n",
    "    word_map['<START>'] = len(word_map) + 1 \n",
    "    word_map['<END>'] = len(word_map) + 1\n",
    "    word_map['<UNK>'] = len(word_map) + 1\n",
    "    word_map['<PAD>'] = 0\n",
    "    \n",
    "    print(\"Number of training images: {0}\".format(num_train_img))\n",
    "    print(\"Number of validation images: {0}\".format(num_val_img))\n",
    "    print(\"Number of testing images: {0}\".format(num_test_img))\n",
    "    return train_img_caps, val_img_caps, test_img_caps, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 6000\n",
      "Number of validation images: 1000\n",
      "Number of testing images: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, word_map = get_train_val_test_splits(image_folder, flickr8k_datasplit, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, split, word_map, base_file_name, captions_per_image):\n",
    "    output_folder = 'data/'\n",
    "    encoded_captions = []\n",
    "    encoded_captions_length = []\n",
    "    start_token = word_map['<START>']\n",
    "    end_token = word_map['<END>']\n",
    "    unknown_token = word_map['<UNK>']\n",
    "    padding_token = word_map['<PAD>']\n",
    "    training_data_file = os.path.join(output_folder, base_file_name + '_' + split + '_images.hdf5')\n",
    "    encoded_captions_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_captions.json')\n",
    "    encoded_captions_length_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_caption_lengths.json')\n",
    "    \n",
    "    print(\"Creating %s data set\" % split)\n",
    "    with h5py.File(os.path.join(output_folder, base_file_name + '_' + split + '_images' + '.hdf5'), 'a') as h:\n",
    "        images = h.create_dataset('images', (len(data), 3, 256, 256), dtype='uint8')\n",
    "        for image_idx ,(image_path, image_captions) in enumerate(data):\n",
    "            \n",
    "            #want to ensure that there are at least certain number of captions per image \n",
    "            #if current image has less than that threshold, then augement the captions\n",
    "            num_captions = len(image_captions)\n",
    "            if num_captions < captions_per_image: \n",
    "                chosen_captions = [random.choice(image_captions) for _ in range(captions_pe_image - num_captions)]\n",
    "                chosen_captions += image_captions\n",
    "            else:\n",
    "                chosen_captions = random.sample(image_captions, k = captions_per_image)\n",
    "            \n",
    "            #for the chosen captions, encode them\n",
    "            \n",
    "            for i, caption in enumerate(chosen_captions):\n",
    "#                 import pdb; pdb.set_trace()\n",
    "                encoded_caption = [word_map.get(w,unknown_token) for w in caption]\n",
    "                assert len(caption) == len(encoded_caption)\n",
    "                padding_for_caption = [padding_token for _ in range(MAX_LENGTH- len(caption))]\n",
    "                encoded_caption = [start_token] + encoded_caption + [end_token] + padding_for_caption\n",
    "                \n",
    "                encoded_captions.append(encoded_caption)\n",
    "                \n",
    "\n",
    "                assert len(encoded_caption) == MAX_LENGTH + 2 \n",
    "                encoded_captions_length.append(len(caption) + 2)\n",
    "            \n",
    "            #resize all images to be 256 x 256 \n",
    "            image = Image.open(image_path)\n",
    "            image_resize = image.resize((256, 256))\n",
    "            image_array = np.asarray(image_resize).transpose(2,0,1) #ensures that 3x256x256\n",
    "            images[image_idx] = image_array\n",
    "            \n",
    "            \n",
    "            assert len(image_array.shape) == 3\n",
    "            \n",
    "        h.attrs['cpi'] = captions_per_image \n",
    "        \n",
    "        print(\"Saving the encoded captions\")\n",
    "        #save the encoded captions and the encoded caption lengths to a json file \n",
    "        with open(encoded_captions_file, 'w') as j:\n",
    "            json.dump(encoded_captions, j)\n",
    "\n",
    "        with open(encoded_captions_length_file, 'w') as j:\n",
    "            json.dump(encoded_captions_length, j)\n",
    "        \n",
    "        print(\"Done creating the dataset for split \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset(test_data, 'test', word_map, 'flickr8k', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, folder, name, split, transform=None):\n",
    "        '''\n",
    "            Create a data set class that will be used when passing into the data loader. \n",
    "        '''\n",
    "        self.split = split\n",
    "        \n",
    "        self.file = h5py.File(os.path.join(folder, name + '_' + self.split + '_images.hdf5'))\n",
    "        self.images = self.file['images']\n",
    "        \n",
    "        self.cpi = self.file.attrs['cpi']\n",
    "        \n",
    "        # load captions\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_captions.json'), 'r') as f:\n",
    "            self.captions = json.load(f)\n",
    "            \n",
    "        # load captions' lenghts\n",
    "        with open(os.path.join(folder, name + '_' + self.split + '_encoded_caption_lengths.json'), 'r') as f:\n",
    "            self.lengths = json.load(f)\n",
    "        \n",
    "                        \n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.FloatTensor(self.images[idx // self.cpi] / 255.0)\n",
    "        \n",
    "        #TODO: not using standard formulation of mean=0, std=1\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        image = normalize(image)\n",
    "        \n",
    "        caption, caption_length = torch.LongTensor(self.captions[idx]), torch.LongTensor([self.lengths[idx]])\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            return image, caption, caption_length\n",
    "        else:\n",
    "            start = self.cpi * (idx // self.cpi)\n",
    "            end = start + self.cpi\n",
    "            \n",
    "            captions = torch.LongTensor(self.captions[start:end])\n",
    "            \n",
    "            \n",
    "            return image, caption, caption_length, captions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MyDataset('data', 'flickr8k', 'train')\n",
    "val_set = MyDataset('data', 'flickr8k', 'val')\n",
    "test_set = MyDataset('data', 'flickr8k', 'test')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=16,shuffle=True, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=16, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim_size=14):\n",
    "        super(Encoder, self).__init__()\n",
    "        resnet = tv.models.resnet101(pretrained=True)\n",
    "\n",
    "        modules = list(resnet.children())[:-2]\n",
    "\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((dim_size, dim_size))\n",
    "        \n",
    "        #TODO: decided not to fine tune blocks 2-4\n",
    "        \n",
    "    def forward(self, images):\n",
    "        # [batch_size, encoded_dim_size, encoded_dim_size, 2048]\n",
    "        return self.pool(self.resnet(images)).permute(0, 2, 3, 1) \n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_encoder, dim_decoder, dim_attention):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attention_encoder = nn.Linear(dim_encoder, dim_attention)\n",
    "        self.attention_decoder = nn.Linear(dim_decoder, dim_attention)\n",
    "        self.both = nn.Linear(dim_attention, 1)\n",
    "    \n",
    "    def forward(self, out_encoder, hidden_decoder):\n",
    "        attention_encoder = self.attention_encoder(out_encoder)\n",
    "        attention_decoder = self.attention_decoder(hidden_decoder)\n",
    "        \n",
    "        weights = self.both(torch.relu(attention_encoder + attention_decoder.unsqueeze(1))).squeeze(2)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        \n",
    "        out = torch.sum((out_encoder * weights.unsqueeze(2)), dim=1)\n",
    "        \n",
    "        return out, weights \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dim_attention, dim_embed, dim_decoder, vocab_size, dim_encoder=2048):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dim_encoder = dim_encoder\n",
    "        self.dim_attention = dim_attention\n",
    "        self.dim_embed = dim_embed\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.attention = Attention(dim_encoder, dim_decoder, dim_attention)\n",
    "        self.embed = nn.Embedding(vocab_size, dim_embed)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.decode_lstm = nn.LSTMCell(dim_embed + dim_encoder, dim_decoder, bias=True)\n",
    "        self.h_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.c_init = nn.Linear(dim_encoder, dim_decoder)\n",
    "        self.f = nn.Linear(dim_decoder, dim_encoder)\n",
    "        \n",
    "        self.fc1 = nn.Linear(dim_decoder, vocab_size)\n",
    "        \n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc1.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def init_hidden(self, out_encoder):\n",
    "        out = out_encoder.mean(dim=1)\n",
    "        \n",
    "        return self.h_init(out), self.c_init(out)\n",
    "    \n",
    "    def forward(self, out_encoder, captions, lengths):\n",
    "        batch_size = out_encoder.size(0)\n",
    "        dim_encoder = out_encoder.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "        \n",
    "        out_encoder = out_encoder.view(batch_size, -1, dim_encoder)\n",
    "        pixels = out_encoder.size(1)\n",
    "        \n",
    "        lengths, ind = lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        out_encoder = out_encoder[ind]\n",
    "        captions = captions[ind]\n",
    "        \n",
    "        embed = self.embed(captions)\n",
    "        \n",
    "        # init hidden state\n",
    "        h, c = self.init_hidden(out_encoder)\n",
    "        \n",
    "        lengths = (lengths-1).tolist()\n",
    "        \n",
    "        predict = torch.zeros(batch_size, max(lengths), vocab_size).to(device)\n",
    "        weights = torch.zeros(batch_size, max(lengths), pixels).to(device)\n",
    "        \n",
    "        for time_step in range(max(lengths)):\n",
    "            batch_t = sum([i > time_step for i in lengths])\n",
    "            \n",
    "            weighted_encoder, alpha = self.attention(out_encoder[:batch_t], h[:batch_t])\n",
    "            \n",
    "            sig = torch.sigmoid(self.f(h[:batch_t]))\n",
    "            weighted_encoder = sig * weighted_encoder\n",
    "            \n",
    "            h, c = self.decode_lstm(torch.cat([embed[:batch_t, time_step, :], weighted_encoder], dim=1),\n",
    "                                    (h[:batch_t], c[:batch_t]))\n",
    "            \n",
    "            output = self.fc1(self.drop(h))\n",
    "            predict[:batch_t, time_step, :] = output\n",
    "            weights[:batch_t, time_step, :] = alpha\n",
    "            \n",
    "        return predict, captions, lengths, weights, ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder(dim_attention=512, dim_embed=512, dim_decoder=512, vocab_size=len(word_map)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = torch.optim.Adam(params=encoder.parameters(), lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(params=decoder.parameters(), lr=4e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    decoder.train()\n",
    "    encoder.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        img_features = encoder(images)\n",
    "        predict, captions, lengths, weights, ind = decoder(img_features, captions, lengths)\n",
    "#         import pdb; pdb.set_trace()\n",
    "        targets = captions[:,1:]\n",
    "        \n",
    "        predict = pack_padded_sequence(predict, lengths, batch_first=True).data\n",
    "        targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "        \n",
    "        loss = criterion(predict, targets)\n",
    "        \n",
    "        loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "        \n",
    "        decoder_optimizer.zero_grad()\n",
    "#         encoder_optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()*sum(lengths)\n",
    "        loss_num += sum(lengths)\n",
    "        if i % 100 == 0: \n",
    "            print(\"Epoch % d [%d/%d], Loss: %f\" % (epoch, i, len(train_loader), loss_sum/loss_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 [0/1875], Loss: 8.893106\n",
      "Epoch  0 [100/1875], Loss: 6.288741\n",
      "Epoch  0 [200/1875], Loss: 5.949400\n",
      "Epoch  0 [300/1875], Loss: 5.725625\n",
      "Epoch  0 [400/1875], Loss: 5.559338\n",
      "Epoch  0 [500/1875], Loss: 5.417818\n",
      "Epoch  0 [600/1875], Loss: 5.322792\n",
      "Epoch  0 [700/1875], Loss: 5.239904\n",
      "Epoch  0 [800/1875], Loss: 5.171329\n",
      "Epoch  0 [900/1875], Loss: 5.109178\n",
      "Epoch  0 [1000/1875], Loss: 5.053124\n",
      "Epoch  0 [1100/1875], Loss: 5.001242\n",
      "Epoch  0 [1200/1875], Loss: 4.957687\n",
      "Epoch  0 [1300/1875], Loss: 4.918577\n",
      "Epoch  0 [1400/1875], Loss: 4.876743\n",
      "Epoch  0 [1500/1875], Loss: 4.842693\n",
      "Epoch  0 [1600/1875], Loss: 4.809957\n",
      "Epoch  0 [1700/1875], Loss: 4.779557\n",
      "Epoch  0 [1800/1875], Loss: 4.753161\n",
      "Validate [0/313], Loss: 4.130768\n",
      "Validate [100/313], Loss: 4.153472\n",
      "Validate [200/313], Loss: 4.173646\n",
      "Validate [300/313], Loss: 4.162018\n",
      "Validate. BLEU-4: 0.121774, Loss: 4.163579\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch)\n",
    "    bleu4 = validate(val_loader, encoder, decoder, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.002452138993924e-232"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    decoder.eval()\n",
    "    encoder.eval()\n",
    "    \n",
    "    refs = []\n",
    "    hypos = []\n",
    "    \n",
    "    loss_sum = 0\n",
    "    loss_num = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths, all_captions) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            \n",
    "            images = encoder(images)\n",
    "            predict, captions, lengths, weights, ind = decoder(images, captions, lengths)\n",
    "            \n",
    "            targets = captions[:,1:]\n",
    "            \n",
    "            scores = predict.clone()\n",
    "            \n",
    "            predict = pack_padded_sequence(predict, lengths, batch_first=True).data\n",
    "            targets = pack_padded_sequence(targets, lengths, batch_first=True).data\n",
    "            \n",
    "            loss = criterion(predict, targets)\n",
    "            \n",
    "            loss += ((1. - weights.sum(dim=1)) ** 2).mean()\n",
    "                     \n",
    "            loss_sum += loss.item()*sum(lengths)\n",
    "            loss_num += sum(lengths)\n",
    "                     \n",
    "            if (i % 100) == 0:\n",
    "                print(\"Validate [%d/%d], Loss: %f\" % (i, len(val_loader), loss_sum/loss_num))\n",
    "            \n",
    "            all_captions = all_captions[ind]\n",
    "            for j in range(all_captions.shape[0]):\n",
    "                image_captions = all_captions[j].tolist()\n",
    "                image_captions = list(map(lambda c : [w for w in c if w not in {word_map['<START>'], \n",
    "                                                                                word_map['<PAD>']}],\n",
    "                                         image_captions))\n",
    "                refs.append(image_captions)\n",
    "            \n",
    "            pred = torch.max(scores, dim=2)[1].tolist()\n",
    "            temp = []\n",
    "            for j, p in enumerate(pred):\n",
    "                temp.append(pred[j][:lengths[j]])\n",
    "            pred = temp\n",
    "            hypos.extend(pred)\n",
    "            \n",
    "            assert len(refs) == len(hypos)\n",
    "            \n",
    "        bleu4 = corpus_bleu(refs, hypos)\n",
    "        \n",
    "        print(\"Validate. BLEU-4: %f, Loss: %f\" % (bleu4, loss_sum/loss_num))\n",
    "        \n",
    "    return bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
