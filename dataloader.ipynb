{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "karpathy_json_path = 'data/dataset_flickr8k.json'\n",
    "MAX_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(karpathy_json_path, 'r') as j:\n",
    "        flickr8k_datasplit = json.load(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "dataset\n"
     ]
    }
   ],
   "source": [
    "for key in flickr8k_datasplit: \n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = 'data/Flicker8k_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_splits(image_folder, image_splits, max_caption_len, min_word_freq):\n",
    "    '''\n",
    "        method will return three lists containing the training, validation, and testing \n",
    "        file paths and captions. Method will also return the word map that will be used for captioning\n",
    "        images. \n",
    "        Parameters: \n",
    "            image_folder: location of the images in the data set, either MSCOCO, Flickr8K, Flickr30k \n",
    "            \n",
    "            image_splits: dictionary that contains the captions for each image and indicates whether \n",
    "            image part of training, validation, or testing set\n",
    "            \n",
    "            max_caption_len: threshold for maximum caption length \n",
    "            \n",
    "            min_word_freq: threshold that determines whether a word will be in word map or not. \n",
    "            \n",
    "            \n",
    "        Output:\n",
    "            train_img_caps: list of tuples containing the training image file path and caption\n",
    "            val_img_caps: list of tuples containing the validation image file path and caption\n",
    "            test_img_caps: list of tuples containing the testing image file path and caption\n",
    "        '''\n",
    "    #storing tuple of path to img and the caption\n",
    "    word_freq = Counter()\n",
    "    train_img_caps = []\n",
    "    val_img_caps = []\n",
    "    test_img_caps = [] \n",
    "    num_train_img, num_val_img, num_test_img = 0, 0, 0 \n",
    "    for img in data['images']:\n",
    "        img_captions = []\n",
    "        for word in img['sentences']:\n",
    "            #check if the caption length is not to long\n",
    "            if len(word['tokens']) <= max_caption_len:\n",
    "                img_captions.append(word['tokens'])\n",
    "            # Update word frequency\n",
    "            word_freq.update(word['tokens'])\n",
    "\n",
    "        #if caption is of length zero move to next image \n",
    "        if not len(img_captions): \n",
    "            continue \n",
    "\n",
    "        img_file_path = os.path.join(image_folder, img['filename'])\n",
    "        #save corresponding files and captions \n",
    "        if img['split'] == 'train':\n",
    "            train_img_caps.append((img_file_path, img_captions))\n",
    "            num_train_img+=1\n",
    "        elif img['split'] == 'val':\n",
    "            val_img_caps.append((img_file_path, img_captions))\n",
    "            num_val_img+=1 \n",
    "        elif img['split'] == 'test':\n",
    "            test_img_caps.append((img_file_path, img_captions))\n",
    "            num_test_img+=1\n",
    "    \n",
    "    #create a limited vocabulary and don't include any word that hasn't appeared \n",
    "    #min_word_freq times\n",
    "    words = [w for w in word_freq if word_freq[w] >= min_word_freq]\n",
    "    min_words = [w for w in word_freq if word_freq[w] < min_word_freq]\n",
    "    word_map = {word: i+1 for i, word in enumerate(words)}\n",
    "    #specify start token, end token, unknown token, and padding token \n",
    "    word_map['<START>'] = len(word_map) + 1 \n",
    "    word_map['<END>'] = len(word_map) + 2\n",
    "    word_map['<UNK>'] = len(word_map) + 3\n",
    "    word_map['<PAD>'] = 0\n",
    "    \n",
    "    print(\"Number of training images: {0}\".format(num_train_img))\n",
    "    print(\"Number of validation images: {0}\".format(num_val_img))\n",
    "    print(\"Number of testing images: {0}\".format(num_test_img))\n",
    "    return train_img_caps, val_img_caps, test_img_caps, word_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 6000\n",
      "Number of validation images: 1000\n",
      "Number of testing images: 1000\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, word_map = get_train_val_test_splits(image_folder, flickr8k_datasplit, 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, split, word_map, base_file_name, captions_per_image):\n",
    "    output_folder = 'data/'\n",
    "    encoded_captions = []\n",
    "    encoded_captions_length = []\n",
    "    start_token = word_map['<START>']\n",
    "    end_token = word_map['<END>']\n",
    "    unknown_token = word_map['<UNK>']\n",
    "    padding_token = word_map['<PAD>']\n",
    "    training_data_file = os.path.join(output_folder, base_file_name + '_' + split + '_images.hdf5')\n",
    "    encoded_captions_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_captions.json')\n",
    "    encoded_captions_length_file = os.path.join(output_folder, base_file_name + '_' + split + '_encoded_caption_lengths.json')\n",
    "    \n",
    "    print(\"Creating training data set\")\n",
    "    with h5py.File(os.path.join(output_folder, base_file_name + '_' + split + '_images' + '.hdf5'), 'a') as h:\n",
    "        images = h.create_dataset('images', (len(data), 3, 256, 256), dtype='uint8')\n",
    "        for image_idx ,(image_path, image_captions) in enumerate(data):\n",
    "            \n",
    "            #want to ensure that there are at least certain number of captions per image \n",
    "            #if current image has less than that threshold, then augement the captions\n",
    "            num_captions = len(image_captions)\n",
    "            if num_captions < captions_per_image: \n",
    "                chosen_captions = [random.choice(image_captions) for _ in range(captions_pe_image - num_captions)]\n",
    "                chosen_captions += image_captions\n",
    "            else:\n",
    "                chosen_captions = random.sample(image_captions, k = captions_per_image)\n",
    "            \n",
    "            #for the chosen captions, encode them\n",
    "            \n",
    "            for i, caption in enumerate(chosen_captions):\n",
    "                encoded_caption = [word_map.get(w,unknown_token) for w in caption]\n",
    "                padding_for_caption = [padding_token for _ in range(MAX_LENGTH- len(caption))]\n",
    "                encoded_caption = [start_token] + encoded_caption + [end_token] + padding_for_caption\n",
    "                \n",
    "                encoded_captions.append(encoded_caption)\n",
    "                encoded_captions_length.append(len(caption) + 2)\n",
    "            \n",
    "            #resize all images to be 256 x 256 \n",
    "            image = Image.open(image_path)\n",
    "            image_resize = image.resize((256, 256))\n",
    "            image_array = np.asarray(image_resize).transpose(2, 0, 1) #ensures that 3x256x256\n",
    "            images[image_idx] = image_array\n",
    "            \n",
    "            \n",
    "            assert len(image_array.shape) == 3\n",
    "            \n",
    "        h.attrs['cpi'] = captions_per_image \n",
    "        \n",
    "        print(\"Saving the encoded captions\")\n",
    "        #save the encoded captions and the encoded caption lengths to a json file \n",
    "        with open(encoded_captions_file, 'w') as j:\n",
    "            json.dump(encoded_captions, j)\n",
    "\n",
    "        with open(encoded_captions_length_file, 'w') as j:\n",
    "            json.dump(encoded_captions_length, j)    \n",
    "        \n",
    "        print(\"Done creating the dataset for split \")\n",
    "            \n",
    "\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training data set\n",
      "Saving the encoded captions\n",
      "Done creating the dataset for split \n"
     ]
    }
   ],
   "source": [
    "create_dataset(train_data, 'train', word_map, 'fliker8k', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/flickr8kdata/'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('data/', 'flickr8kdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
